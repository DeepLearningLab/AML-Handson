{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# [PyTorch] Azure MLで学習済みモデルの作成からデプロイまで\n\nこのチュートリアルでは、`Azure Machine Learning（Azure ML）Python SDK`を使用して、モデルのトレーニング、ハイパーパラメーターの調整、およびデプロイを行います。  \n※ディープラーニングのフレームワークには`PyTorch`を使用します。  \n\n\n問題設定は[Transfer Learningチュートリアル](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)から、  Transfer Learning（転移学習）を使用してアリとハチの画像分類になります。  \n\n## 転移学習とは？  \n転移学習とは学習済みモデルを使用して（ネットワークの構造と重みの再利用）、学習を行うことをさします。  \n類似のものとしてファインチューニングがありますが、学習済みモデルのネットワークの学習を行うのがファインチューニングになります。  \n転移学習では学習済みモデルのネットワーク自体の学習は行いません。（出力前の全結合層のみを学習させるのが一般的）  "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 環境構築\n\n環境は「[Azure Machine Learning Services](https://docs.microsoft.com/ja-jp/azure/machine-learning/service/) ワークスペース」にある`Azure Notebooks`を使用します。  \nPythonの実行環境や、Azure ML Servicesを使用に必要な[Azure ML Python SDK](https://docs.microsoft.com/ja-jp/python/api/overview/azure/ml/intro?view=azure-ml-py)は既にインストールされています。　  \n\nこの画面の右上の Kernel が `Python 3.6` になっていることを確認します。異なる場合は、[Kernel] メニューから、変更してください。"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### バージョンの確認"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import azureml.core\n\nprint(\"This notebook was created using version 1.0.15 of the Azure ML SDK\")\nprint(\"You are currently using version\", azureml.core.VERSION, \"of the Azure ML SDK\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "このコードは、SDK `1.0.10`か`1.0.15` で動作確認しています。もし、SDKの更新を行う場合は、以下を実行して、SDKを更新してください。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install --upgrade azureml-sdk[notebooks,automl] azureml-dataprep",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Python SDK をアップデートした場合は、JupyterNotebook の **Kernel** を **Restart** して最新版をロードします。"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Azrue Notebook では既にAzureML Python SDKが準備されているため、インストールする必要はありません。  "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### ワークスペースの初期化\n\n**00.configuration.ipynb を実行して、`config.json` ファイルを作成済みの場合は、このセルはスキップして次に進んでください。**\n\n[ワークスペース](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace)の初期化を行います。  \n`Workspace.from_config()`は`config.json`ファイルを参照してワークスペースを初期化します。 \n\n詳細は[こちら](https://docs.microsoft.com/ja-jp/azure/machine-learning/service/how-to-configure-environment#workspace) を参照してください。\n\n#### configファイルの編集\n\n`config.json`ファイルは基本的に自動でこのように設定を反映されます。  \n自身で設定する際には下記のように編集します。  \n\n```json\n\n{\n    \"subscription_id\": \"サブスクリプションID\",\n    \"resource_group\": \"リソースグループ名\",\n    \"workspace_name\": \"ワークスペース名\"\n}\n\n```\n\n上記の情報はAzure Portalの画面から確認することができます。  \nでは、実行して、ワークスペースの初期化を行います。  \n\n初期化を行う時に、サインインを要求されるので、表示されるコードをコピーして、URLをクリックします。  \n遷移先の画面でコピーしたコードを入力することによって、サインインが完了します。  "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.workspace import Workspace\n\nws = Workspace.from_config()\nprint('Workspace name: ' + ws.name, \n      'Azure region: ' + ws.location, \n      'Resource group: ' + ws.resource_group, sep = '\\n')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## コンピューティング ターゲットの設定\n\n[コンピューティングターゲット](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target)を作成する必要があります。このチュートリアルではAzure ML managed compute ([AmlCompute](https://docs.microsoft.com/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute))を使用します。  \n（コンピューティングターゲットは計算を実行する場所を決定するようなイメージです。）\n\n※AmlComputeの作成には約5分かかります。  \nその名前のAmlComputeが既にワークスペースにある場合、このコードは作成プロセスをスキップします。\n\n他のAzureサービスと同様に、Azure Machine Learningサービスに関連する特定のリソース（AmlComputeなど）には制限があります。  \nデフォルトの制限と、より多くのクォータを要求する方法についての[この記事](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas)を読んでください。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.compute_target import ComputeTargetException\n\n# choose a name for your cluster\ncluster_name = \"gpucluster\"\n\ntry:\n    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n    print('Found existing compute target.')\nexcept ComputeTargetException:\n    print('Creating a new compute target...')\n    compute_config = AmlCompute.provisioning_configuration(vm_size=\"Standard_NC6\", ##  Standard_NC6s_v3\n                                                       min_nodes=1,\n                                                       max_nodes=1,\n                                                       vm_priority='lowpriority') ## vm_priority='lowpriority' | `dedicated'\n\n    # create the cluster\n    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n\n    compute_target.wait_for_completion(show_output=True)\n\n# Use the 'status' property to get a detailed status for the current cluster. \nprint(compute_target.status.serialize())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "上記のコードはGPUクラスターを作成します。  \nコードの中身を確認します。  \n`AmlCompute.provisioning_configuration()`でコンピューティング ターゲットの設定を行うことができます。  \n詳細は[こちらの公式ドキュメント](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.compute.amlcompute(class)?view=azure-ml-py)を確認してください。  \n\n\n### 仮想マシンのサイズの変更\n\n代わりにCPUクラスタを作成したい場合は、 `STANDARD_D2_V2`のように` vm_size`パラメータに異なるVMサイズを指定してください。  \n\nCPUのVMサイズは[こちらの公式ドキュメント](https://docs.microsoft.com/ja-jp/azure/virtual-machines/linux/sizes-general)を確認してください。  \nGPUのVMサイズは[こちらの公式ドキュメント](https://docs.microsoft.com/ja-jp/azure/virtual-machines/linux/sizes-gpu)を確認してください。  \n\n今回はNVIDIAのTesla K80が1枚の仮想マシン`STANDARD_NC6`を使用します。  \n計算リソースを増やすためにはNCの他のシリーズを使用するもしくはGPUの枚数を増やすことによって行うことが可能です。  \n\n\n### 仮想マシンの割り当ての設定\n\nAzureの仮想マシンのプライオリティ（優先度）を選択することができます。  \n選択肢は`dedicated`または`lowpriority`の２つから選択することができます。  \n（デフォルトでは`dedicated`が選択されています。）  \n\n`dedicated`は問題なく仮想マシンが割り当てられますが、`lowpriority`は価格が安い代わりに割り込みが入る可能性などいくつかデメリットがあります。  \nしかし、価格が約8割ほど安くなるのは大きなメリットです。  \n\n\n### クラスターのノード数の設定\n\n`max_nodes`でコンピューティングでジョブを実行中に自動スケールアップする最大ノード数を指定することが可能です。  \nノード数はVMの数を表すため最大数が増えると計算リソースが増えますが、同時に発生する料金も増えます。  "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# (Option) Data Science VM の設定\n\n**AmlCompute の作成ができた方は **スキップ** してください!!!実行すると、AmlCompute設定が**上書き**されてしまいます。**\n\nAzure Machine Learning Services では、Data Science VMのGPUインスタンス版など、リモートの仮想マシンを学習の実行環境に設定することもできます。\n\nAzure 無償トライアルなどで、GPUインスタンスのクォータ引き上げが出来ない場合もありますのでご注意ください。\n\n[こちら](https://docs.microsoft.com/ja-jp/azure/machine-learning/service/how-to-set-up-training-targets#vm) のドキュメントを参照して、*事前*に Data Science VM を作成してから、以下を実行してください。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.compute import RemoteCompute, ComputeTarget\n\n# Create the compute config \ncompute_target_name = 'attachdsvm'\nattach_config = RemoteCompute.attach_configuration(address='<FQDN もしくは IP Address>',\n                                                 ssh_port=22,\n                                                 username='User Name',\n                                                 password='Password')\n\n# Attach the compute\ncompute_target = ComputeTarget.attach(ws, compute_target_name, attach_config)\n\ncompute_target.wait_for_completion(show_output=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## GPUクラスタを使用しての学習の実行\n\nリモートコンピューティングクラスタを使用して学習する準備が整いました。  \nPytorchでの学習のスクリプトと学習用のデータを準備します。  \n\n今回は事前に準備されたものを使用します。　　"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### プロジェクトディレクトリの作成\n\n学習実行に必要なコードを格納するディレクトリを作成します。  \nこのディレクトリには学習を実行するコードと、それに依存関係のファイルなどを格納するする必要があります。  "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\n\nproject_folder = './pytorch-hymenoptera'\nos.makedirs(project_folder, exist_ok=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "今回は`pytorch-hymenoptera`という名前のプロジェクトフォルダを作成しました。  "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### データセットの準備\n\n今回は[こちら](https://download.pytorch.org/tutorial/hymenoptera_data.zip)のデータセットを使用します。  \n（ダウンロードの必要はありません）\n\n\nこちらにはアリとミツバチの画像それぞれ約120個ずつの訓練データ、75個の検証データが含まれています。  \n学習用のスクリプトである`pytorch_train.py`内にデータセットをダウンロードして取得するコードがあるため、  \nこちらのデータは今回はダウンロードして準備する必要はありません。  "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "\n### 学習用スクリプトの準備\n\n学習用のスクリプトは用意されている`pytorch_train.py`を使用します。  \n\n\n### スクリプトの確認\n\n今回使用するスクリプトはこちらになります。  \n`pytorch_train.py`  \nこちらのスクリプトの詳細の説明は行いませんが、実行内容としては下記の4ステップになります。    \n\n1. データのダウンロード\n2. 必要な前処理の適応\n3. 学習の実行\n4. 結果の取得"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile pytorch_train.py  \n\nfrom __future__ import print_function, division\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torchvision import datasets, models, transforms\nimport numpy as np\nimport time\nimport os\nimport copy\nimport argparse\n\nfrom azureml.core.run import Run\n# get the Azure ML run object\nrun = Run.get_context()\n\n\ndef load_data(data_dir):\n    \"\"\"Load the train/val data.\"\"\"\n\n    # Data augmentation and normalization for training\n    # Just normalization for validation\n    data_transforms = {\n        'train': transforms.Compose([\n            transforms.RandomResizedCrop(224),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]),\n        'val': transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]),\n    }\n\n    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n                                              data_transforms[x])\n                      for x in ['train', 'val']}\n    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n                                                  shuffle=True, num_workers=4)\n                   for x in ['train', 'val']}\n    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n    class_names = image_datasets['train'].classes\n\n    return dataloaders, dataset_sizes, class_names\n\n\ndef train_model(model, criterion, optimizer, scheduler, num_epochs, data_dir):\n    \"\"\"Train the model.\"\"\"\n\n    # load training/validation data\n    dataloaders, dataset_sizes, class_names = load_data(data_dir)\n\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                scheduler.step()\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n            # log the best val accuracy to AML run\n            run.log('best_val_acc', np.float(best_acc))\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed // 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model\n\n\ndef fine_tune_model(num_epochs, data_dir, learning_rate, momentum):\n    \"\"\"Load a pretrained model and reset the final fully connected layer.\"\"\"\n\n    # log the hyperparameter metrics to the AML run\n    run.log('lr', np.float(learning_rate))\n    run.log('momentum', np.float(momentum))\n\n    model_ft = models.resnet18(pretrained=True)\n    num_ftrs = model_ft.fc.in_features\n    model_ft.fc = nn.Linear(num_ftrs, 2)  # only 2 classes to predict\n\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    model_ft = model_ft.to(device)\n\n    criterion = nn.CrossEntropyLoss()\n\n    # Observe that all parameters are being optimized\n    optimizer_ft = optim.SGD(model_ft.parameters(),\n                             lr=learning_rate, momentum=momentum)\n\n    # Decay LR by a factor of 0.1 every 7 epochs\n    exp_lr_scheduler = lr_scheduler.StepLR(\n        optimizer_ft, step_size=7, gamma=0.1)\n\n    model = train_model(model_ft, criterion, optimizer_ft,\n                        exp_lr_scheduler, num_epochs, data_dir)\n\n    return model\n\n\ndef download_data():\n    \"\"\"Download and extract the training data.\"\"\"\n    import urllib\n    from zipfile import ZipFile\n    # download data\n    data_file = './hymenoptera_data.zip'\n    download_url = 'https://download.pytorch.org/tutorial/hymenoptera_data.zip'\n    urllib.request.urlretrieve(download_url, filename=data_file)\n\n    # extract files\n    with ZipFile(data_file, 'r') as zip:\n        print('extracting files...')\n        zip.extractall()\n        print('finished extracting')\n        data_dir = zip.namelist()[0]\n\n    # delete zip file\n    os.remove(data_file)\n    return data_dir\n\n\ndef main():\n    print(\"Torch version:\", torch.__version__)\n\n    # get command-line arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--num_epochs', type=int, default=25,\n                        help='number of epochs to train')\n    parser.add_argument('--output_dir', type=str, help='output directory')\n    parser.add_argument('--learning_rate', type=float,\n                        default=0.001, help='learning rate')\n    parser.add_argument('--momentum', type=float, default=0.9, help='momentum')\n    args = parser.parse_args()\n\n    data_dir = download_data()\n    print(\"data directory is: \" + data_dir)\n    model = fine_tune_model(args.num_epochs, data_dir,\n                            args.learning_rate, args.momentum)\n    os.makedirs(args.output_dir, exist_ok=True)\n    torch.save(model, os.path.join(args.output_dir, 'model.pt'))\n\n\nif __name__ == \"__main__\":\n    main()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Azure MLの学習結果をログに保存\n\n上記のコードにはAzure MLの環境で学習を実行し、結果を追跡するにはいくつかのAzure MLコードが追記されています。  \n詳細は[こちらの公式のドキュメント](https://docs.microsoft.com/ja-jp/azure/machine-learning/service/how-to-track-experiments)を確認してください。  \n今回記述されている内容をそれぞれ確認しましょう。  \n\n学習経過には`Azure ML Run`オブジェクトを使用することによってアクセスすることができます。    \n上記のコード内で設定を行なっている部分を確認しましょう。  \n\n```Python\nfrom azureml.core.run import Run\nrun = Run.get_context()\n```\n\nさらに`learning rate`、`momentum`のパラメータ、検証データに対する最高のAccuracy（正解率）のログも取得します。  \n\n```Python\nrun.log('lr', np.float(learning_rate))\nrun.log('momentum', np.float(momentum))\n\nrun.log('best_val_acc', np.float(best_acc))\n```\n\nハイパーパラメータの調整を行う際にこちらのログは重要な役割を果たします。  \nこちらのスクリプトを先ほど作成した、作業ディレクトリに保存しておきます。  "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import shutil\n\nshutil.copy('pytorch_train.py', project_folder)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Experimentの作成\nワークスペースですべての実行結果を追跡するために[Experiment](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#experiment) を作成します。  "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Experiment\n\nexperiment_name = 'pytorch-hymenoptera'\nexperiment = Experiment(ws, name=experiment_name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### PyTorch estimatorの作成\n\nAzure ML SDK の PyTorch Estimator を使用すると、単一ノードと分散の両方の実行について、PyTorchトレーニングジョブを簡単に送信できます。   \nPyTorch Estimatorの詳細については、[こちら](https://docs.microsoft.com/azure/machine-learning/service/how-to-train-pytorch)を参照してください。次のコードは単一ノードの PyTorch ジョブを定義します。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.dnn import PyTorch\n\nscript_params = {\n    '--num_epochs': 30,\n    '--output_dir': './outputs'\n}\n\nestimator = PyTorch(source_directory=project_folder, \n                    script_params=script_params,\n                    compute_target=compute_target,\n                    entry_script='pytorch_train.py',\n                    use_gpu=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "`scripti_params`に訓練に必要な引数を渡す必要があります。　　\n\n\n#### script_params\n\n`script_params`は` entry_script`で指定しているスクリプトに必要な引数を渡す辞書型のオブジェクトです。  \n今回の設定は下記になります。  \n- `--num_epochs`:`30`→エポック数を30に設定\n- `'--output_dir': './outputs'`→学習の実行履歴を保存するディレクトリの指定\n\nこの出力ディレクトリである `./ output`はAzure ML上で特別に扱われます。  \nこのディレクトリ内の情報は全て実行履歴の一部としてワークスペースにアップロードされ、リモート実行が終了してもアクセス可能です。\n\n#### データストアからデータの読み込み\n\n訓練時にデータを読み込む必要がある場合はデータストアと呼ばれる場所にデータをUploadし、そこからデータを読み込む必要があります。  \nその方法については[こちらの公式ドキュメント](https://docs.microsoft.com/ja-jp/azure/machine-learning/service/how-to-access-data)を確認してください。  \n（データストアはワークスペースのリソースを作成した段階で使用可能になります。）  \n\n\n#### GPUの使用\n\nAzure VMのGPUをトレーニングに活用するには、`use_gpu = True`に設定します。\nCPUしか利用できない場合は、このパラメーターを削除するか、`user_gpu=False` に設定しなおします。\n\n各種パラメーター:\n[PyTorch class](https://docs.microsoft.com/ja-jp/python/api/azureml-train-core/azureml.train.dnn.pytorch?view=azure-ml-py)"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": "### ジョブの実行\n\nEstimatorオブジェクトを送信してExperimentを実行します。  \nこの実行は非同期です。\n\n学習の実行には下記の4ステップがあります。  \n\n1. 準備：Chainer Estimater で指定されたPython環境に合わせてdockerイメージが作成され、それがワークスペースのAzure Container Registryにアップロードされます。このステップはPython環境ごとに一度だけ起こります。（その後の実行のためにコンテナはキャッシュされます。）画像の作成とアップロードには約5分かかります。ジョブの準備中、ログは実行履歴にストリーミングされ、イメージ作成の進行状況を監視するために表示できます。\n\n2. スケーリング：計算をスケールアップする必要がある場合（つまり、バッチAIクラスターで現在実行可能な数より多くのノードを実行する必要がある場合）、クラスターは必要な数のノードを使用可能にするためにスケールアップを試みます。スケーリングは通常約5分かかります。\n\n3. 実行中：スクリプトフォルダ内のすべてのスクリプトがコンピューティングターゲットにアップロードされ、データストアがマウントまたはコピーされてentry_scriptが実行されます。ジョブの実行中は、stdoutと./logsフォルダが実行履歴にストリーミングされ、実行の進行状況を監視するために表示できます。\n\n4. 後処理：実行の./outputsフォルダが実行履歴にコピーされます。\n\n環境にもよりますが、20分程度かかります。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run = experiment.submit(estimator)\nprint(run)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# to get more details of your run\nprint(run.get_details())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 実行経過の確認\n\nJupyter Notebookのウィジェットを使用して実行の進行状況を監視できます。  \n実行依頼と同様に、ウィジェットは非同期で、ジョブが完了するまで10〜15秒ごとにライブアップデートを行います。\n\nまた、このウィジェットの `Status` が `Queued` になると。実際にGPUマシンが自動的に作成されます。\n\nウィジェットの一番下にある `Click here to see the run in Azure portal` から、`コンピューティング` の状態も併せて確認してください。[最新の情報を更新] を押してリフレッシュすることをお勧めします。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\n\nRunDetails(run).show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "また、スクリプトがトレーニングを完了するまでブロックしてから、学習結果を確認することも可能です。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run.wait_for_completion(show_output=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 学習済みモデルのデプロイ\n\n学習済みモデルが作成できました。  \n続いてそのモデルをAzureにデプロイします。   \n今回はモデルを[Azure Container Instances](https://docs.microsoft.com/en-us/azure/container-instances/)（ACI）にWebサービスとしてデプロイします。   \nAzure MLを使用してモデルを展開する方法の詳細については、[こちら](https://docs.microsoft.com/azure/machine-learning/service/how-to-deploy-and-where)を参照してください。"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 学習済みモデルの保存\n\n`run.register_model`を使用すると学習済みモデルを保存することが可能です。  "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model = run.register_model(model_name='pytorch-hymenoptera', model_path='outputs/model.pt')\nprint(model.name, model.id, model.version, sep = '\\t')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### スコアリングスクリプトの作成\n\nまず、Webサービスに呼び出されるスコアリングスクリプトを作成します。  \nスコアリングスクリプトには、2つの関数が必要になります。\n\n- `init（）`：この関数では、通常モデルを `global`オブジェクトにロードします。この関数はDockerコンテナが起動されたときに一度だけ実行されます。\n- `run（input_data）`：この関数では、新たな入力データ対して学習済みモデルを使用して推論を実行します。通常は入力と出力は通常シリアライゼーションとデシリアライゼーションのフォーマットとしてJSONを使用しますが、他のフォーマットも使用することが可能です。\n\n今回は準備されている`pytorch_score.py`を使用します。  \nまた用意されているテスト用の画像ファイルを使用して推論を実行します。  \n独自のスコアリングスクリプトを書くときは、Webサービスを実行する前にまずローカルでテストすることを忘れないでください。  \n\n使用するスクリプトは下記になります。  "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile pytorch_score.py\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms\nimport json\n\nfrom azureml.core.model import Model\n\n\ndef init():\n    global model\n    model_path = Model.get_model_path('pytorch-hymenoptera')\n    model = torch.load(model_path, map_location=lambda storage, loc: storage)\n    model.eval()\n\n\ndef run(input_data):\n    input_data = torch.tensor(json.loads(input_data)['data'])\n\n    # get prediction\n    with torch.no_grad():\n        output = model(input_data)\n        classes = ['ants', 'bees']\n        softmax = nn.Softmax(dim=1)\n        pred_probs = softmax(output).numpy()[0]\n        index = torch.argmax(output, 1)\n\n    result = {\"label\": classes[index], \"probability\": str(pred_probs[index])}\n    return result",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 環境ファイルを作成する\n\nスコアリングスクリプトのすべてのパッケージ依存関係を指定する環境ファイル（ `myenv.yml`）を作成する必要があります。このファイルは、Azure MLによってこれらのすべての依存関係がDockerイメージにインストールされるようにするために使用されます。この場合、 `azureml-core`、` torch`、そして `torchvision`が必要になります。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.conda_dependencies import CondaDependencies \n\nmyenv = CondaDependencies.create(pip_packages=['azureml-defaults', 'torch', 'torchvision'])\n\nwith open(\"myenv.yml\",\"w\") as f:\n    f.write(myenv.serialize_to_string())\n    \nprint(myenv.serialize_to_string())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Dockerイメージの設定\n\nACIコンテナーを構築するために使用するDockerイメージを構成します。  \n詳細については[こちらの公式ドキュメント](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.image.containerimage?view=azure-ml-py)を確認してください。  \n\n併せて Azure Portal の `イメージ` で、指定したコンテナーイメージが作成されているもの確認してください。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.image import ContainerImage\n\nimage_config = ContainerImage.image_configuration(execution_script='pytorch_score.py', \n                                                  runtime='python', \n                                                  conda_file='myenv.yml',\n                                                  description='Image with hymenoptera model')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### ACIコンテナの設定\n\nデプロイのための準備がほぼ整いました。   \nACIコンテナに必要なCPUの数とギガバイトのRAMを指定するためのデプロイメント構成ファイルを作成します。  \nそれは作成したモデルに依存しますが、一般的なモデルではデフォルトの `1`コアと` 1`ギガバイトのRAMで十分なケースが多いです。  "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.webservice import AciWebservice\n\naciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n                                               memory_gb=1, \n                                               tags={'data': 'hymenoptera',  'method':'transfer learning', 'framework':'pytorch'},\n                                               description='Classify ants/bees using transfer learning with PyTorch')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Container Instances にデプロイする\n\n最後に、登録したモデルからWebサービスをデプロイしましょう。  \n前の手順で作成したACI設定ファイルとイメージ設定ファイルを使用してWebサービスをデプロイします。  \n\nリストの中の `model`オブジェクトを` models`パラメータに渡します。  \n複数の登録済みモデルをデプロイする場合は、このリストに他のモデルを追加してください。　　"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%time\nfrom azureml.core.webservice import Webservice\n\nservice_name = 'aci-hymenoptera'\nservice = Webservice.deploy_from_model(workspace=ws,\n                                       name=service_name,\n                                       models=[model],\n                                       image_config=image_config,\n                                       deployment_config=aciconfig,)\n\nservice.wait_for_deployment(show_output=True)\nprint(service.state)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "通常デプロイには7~8分かかります。  \n下記のように表示されればデプロイが成功しています。  \n\n```\nSucceededACI service creation operation finished, operation \"Succeeded\"\n\n```\n\n#### デプロイがうまくいかない場合\n\nもし、何らかの理由でデプロイが失敗して再デプロイする必要がある場合は、必ずサービスを`service.delete()`で削除してください。一番下のセルにあります。\n\n**また、デプロイに問題が発生した場合、まず下記のコマンドを実行して、サービスからログを取得しましょう。**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "service.get_logs()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "RESTクライアント呼び出しを受け付けるWebサービスのHTTPエンドポイントを取得します。  \nこのエンドポイントは、Webサービスをテストしたい、またはそれをアプリケーションに統合したい人と共有することができます。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(service.scoring_uri)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## デプロイされたサービスをテストする\n\n最後に、デプロイしたWebサービスをテストしましょう。  \nデータをJSON文字列としてACIでホストされているWebサービスに送信し、SDKの `run` APIを使用してサービスを呼び出します。  \nここで、検証データからイメージを取得して推論を実行します。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os, json\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nplt.imshow(Image.open('./data/test_img.jpg'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "画像データに対して学習時と同じ前処理を適応し、推論が実行できる状態に変更します。  "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import torch\nfrom torchvision import transforms\n    \ndef preprocess(image_file):\n    \"\"\"Preprocess the input image.\"\"\"\n    data_transforms = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n\n    image = Image.open(image_file)\n    image = data_transforms(image).float()\n    image = torch.tensor(image)\n    image = image.unsqueeze(0)\n    return image.numpy()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "デプロイしたAPIを使用して推論を実行します。  "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "input_data = preprocess('./data/test_img.jpg')\nresult = service.run(input_data=json.dumps({'data': input_data.tolist()}))\nprint(result)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "うまく推論ができていることが確認できました。  \nこのデプロイされたモデルに関してはAzure Portalの「デプロイ」タブから詳細情報について確認することができます。  \n\nこれでAzure Machine Learningの基礎的な使用方法が理解できました。  "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 後片付け\n\nWebサービスが不要になったら、API呼び出しで簡単に削除できます。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "service.delete()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "続いては一緒に手持ちのデータを使用して学習を行う方法を確認します。  \nまた今回は行わなかったハイパーパラメータの調整方法もご紹介します。  "
    }
  ],
  "metadata": {
    "authors": [
      {
        "name": "minxia"
      }
    ],
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "msauthor": "minxia"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}