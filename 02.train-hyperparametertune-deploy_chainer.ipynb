{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# [Chainer] Azure MLで、自分のファイルを使って、ハイパーパラメーターの調整まで\n\n先ほどの解説では用意されてものを使用してきました。  \nこのハンズオンでは自身のデータを用いて、ハンズオン終了後にもAzure MLを使いこなすようになることを目指します。 また、Chainer を使用して学習を行います。\n\nこのハンズオンでは下記の内容を行います。   \n- モデルのトレーニング\n- ハイパーパラメーターの調整\n- (開発途中)デプロイ"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import azureml.core\n\nprint(\"This notebook was created using version 1.0.15 of the Azure ML SDK\")\nprint(\"You are currently using version\", azureml.core.VERSION, \"of the Azure ML SDK\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "ここでは、Python SDK の Chainer Estimator を使います。Chainer Estimator は SDK の **1.0.15** から導入されています。\n\nAzure Notebook の Azure Machine Learning Serivice SDK のバージョンが 1.0.15よりも古い場合は、以下のコマンドで最新版にアップデートしてください。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install --upgrade azureml-sdk[notebooks,automl] azureml-dataprep",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Azure Machine Services の Workspace へのログインと接続\n\nAzure Machine Learning service へ接続を行います。アクセス権設定は、Azure Machine Learning services 作成時に自動的に行われています。作成した方はもちろん、その Workspace へアクセスできます。\n\nAzure Notebook からは、シングルサインオンができます。以下のセルを実行すると、ログイン用のダイアログが表示されます。そのままログインをして先に進んでください。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.workspace import Workspace\n\nws = Workspace.from_config()\nprint('Workspace name: ' + ws.name, \n      'Azure region: ' + ws.location, \n      'Resource group: ' + ws.resource_group, sep = '\\n')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## コンピューティング ターゲットの設定\n\n[コンピューティングターゲット](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target)を作成する必要があります。このチュートリアルではAzure ML managed compute ([AmlCompute](https://docs.microsoft.com/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute))を使用します。  \n（コンピューティングターゲットは計算を実行する場所を決定するようなイメージです。）\n\n※AmlComputeの作成には約5分かかります。  \nその名前のAmlComputeが既にワークスペースにある場合、このコードは作成プロセスをスキップします。\n\n他のAzureサービスと同様に、Azure Machine Learningサービスに関連する特定のリソース（AmlComputeなど）には制限があります。  \nデフォルトの制限と、より多くのクォータを要求する方法についての[この記事](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas)を読んでください。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.compute_target import ComputeTargetException\n\n# choose a name for your cluster\ncluster_name = \"gpucluster\"\n\ntry:\n    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n    print('Found existing compute target.')\nexcept ComputeTargetException:\n    print('Creating a new compute target...')\n    compute_config = AmlCompute.provisioning_configuration(vm_size=\"Standard_NC6\", ## Standard_NC6s_v3\n                                                       min_nodes=1,\n                                                       max_nodes=1,\n                                                       vm_priority='lowpriority') ## vm_priority='lowpriority' | `dedicated'\n\n    # create the cluster\n    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n\n    compute_target.wait_for_completion(show_output=True)\n\n# Use the 'status' property to get a detailed status for the current cluster. \nprint(compute_target.status.serialize())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# (Option) Data Science VM の設定\n\n**AmlCompute の作成ができた方は **スキップ** してください!!!実行すると、AmlCompute設定が**上書き**されてしまいます。**\n\nAzure Machine Learning Services では、Data Science VMのGPUインスタンス版など、リモートの仮想マシンを学習の実行環境に設定することもできます。\n\nAzure 無償トライアルなどで、GPUインスタンスのクォータ引き上げが出来ない場合もありますのでご注意ください。\n\n[こちら](https://docs.microsoft.com/ja-jp/azure/machine-learning/service/how-to-set-up-training-targets#vm) のドキュメントを参照して、*事前*に Data Science VM を作成してから、以下を実行してください。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.compute import RemoteCompute, ComputeTarget\n\n# Create the compute config \ncompute_target_name = 'attachdsvm'\nattach_config = RemoteCompute.attach_configuration(address='<FQDN もしくは IP Address>',\n                                                 ssh_port=22,\n                                                 username='User Name',\n                                                 password='Password')\n\n# Attach the compute\ncompute_target = ComputeTarget.attach(ws, compute_target_name, attach_config)\n\ncompute_target.wait_for_completion(show_output=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## データストアにデータをUpload\n\nリモートコンピューティングクラスタを使用して学習する準備が整いました。  \nまずは学習用のデータを準備します。  \n今回は手持ちのデータを使用するため、データを[データストア](https://docs.microsoft.com/azure/machine-learning/service/how-to-access-data)にUploadする必要があります。  \nデータストアは、データを格納できる場所であり、データをマウントするか、計算対象にデータをコピーすることによってRunにアクセスできるようになります。  \nデータストアは、Azure Blob StorageまたはAzure File Shareのいずれかでバックアップできます（ADLSは将来サポートされる予定です）。  \n単純なデータ処理のために、データがまだBlob StorageまたはFile Shareにない場合は、各ワークスペースに使用可能なデフォルトのデータストアが用意されています。\n\n### プロジェクトディレクトリの作成\n\n学習実行に必要なコードを格納するディレクトリを作成します。  \nこのディレクトリには学習を実行するコードと、それに依存関係のファイルなどを格納するする必要があります。  "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\n\nproject_folder = './chainer-doc_cat'\nos.makedirs(project_folder, exist_ok=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "今回は`chainer-doc_cat`という名前のプロジェクトフォルダを作成しました。  "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### データストアの準備\n\n今回の問題設定は犬と猫の分類です。  \n先ほどはWeb上に準備されたデータを取得して学習を行いましたが、今回は自身のデータを使用することを想定して学習を行います。  \n今回はデフォルトで設定されているデータストアを使用します。  \n\nGithub からクローンした際に、`/data/doc_cat` フォルダーに、学習用のデータセットも一緒にコピーしています。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "datastores = ws.datastores\nfor name, dss in datastores.items():\n    print(name, dss.datastore_type)\n    \nds = ws.get_default_datastore()\nprint(ds.name, ds.datastore_type, ds.account_name, ds.container_name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "この次のステップでは、トレーニングとテストセットをワークスペースのデフォルトデータストアにアップロードします。  \nこれは後で`AmlCompute`クラスターにマウントしてトレーニングを行います。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%time\nds.upload(src_dir='./data', target_path='data', overwrite=True, show_progress=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 学習スクリプトの作成\n\n基本的には先ほど使用したものを用います。  \nデータの読み込みの部分が持っているデータを扱うため、データをダウンロードする必要がなくなります。  \n\nスクリプトは下記のディレクトリに格納しておきます。  "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile chainer-doc_cat/train.py\n\nimport numpy as np\nimport chainer\nimport chainer.links as L\nimport chainer.functions as F\nimport cupy\nfrom glob import glob\nfrom chainer.datasets import TransformDataset\nfrom zipfile import ZipFile\nimport time\nimport os\nimport copy\nimport argparse\nimport random\nfrom model import GoogleNet\n\nimport chainermn\n\nfrom azureml.core.run import Run\n# get the Azure ML run object\nrun = Run.get_context()\n\n# データの読み込み関数\ndef get_data(data_folder):\n    data_file = os.path.join(data_folder, 'data/dog-cat.zip')\n#    data_file = data_folder\n    \n    print('training dataset is stored here:', data_file)\n\n    # extract files\n    with ZipFile(data_file, 'r') as zipfile:\n        try:\n            print('extracting files...')\n            zipfile.extractall()\n            print('finished extracting')\n            data_dir = zipfile.namelist()[0]\n        except:\n            data_dir = zipfile.namelist()[0]\n\n    return data_dir\n\n# データに対する前処理用の関数\ndef transform(inputs):\n    img, label = inputs\n    img = img.transpose(1,2,0)\n    img = L.model.vision.googlenet.prepare(img)\n    img, label = cupy.asarray(img), cupy.asarray(label)\n    return img, label\n\n# データセットの作成と前処理の適応\ndef prepare_data(data_dir):\n    labels = {'dog':0, 'cat':1}\n    # パスの取得\n    train_dog = [(path, labels['dog']) for path in glob('{}{}/{}/*'.format(data_dir, 'train','dog'))]\n    train_cat = [(path, labels['cat']) for path in glob('{}{}/{}/*'.format(data_dir, 'train','cat'))]\n    val_dog = [(path, labels['dog']) for path in glob('{}{}/{}/*'.format(data_dir, 'val','dog'))]\n    val_cat = [(path, labels['cat']) for path in glob('{}{}/{}/*'.format(data_dir, 'val','cat'))]\n    # データセットの作成\n    train = chainer.datasets.LabeledImageDataset(train_dog+train_cat)\n    valid= chainer.datasets.LabeledImageDataset(val_dog+val_cat)\n    # 前処理の適応\n    train = TransformDataset(train, transform)\n    valid = TransformDataset(valid, transform)\n\n    return train, valid\n\n# 乱数のシード固定用の関数\ndef reset_seed(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    if chainer.cuda.available:\n        chainer.cuda.cupy.random.seed(seed)\n\n# 学習用の関数\ndef train_model(model, comm, optimizer, data_dir, batchsize, num_epochs, learning_rate, lr_shift_timing, lr_shift_rate):\n    train, valid = prepare_data(data_dir)\n    train = chainermn.scatter_dataset(train, comm, shuffle=True)\n    valid = chainermn.scatter_dataset(valid, comm, shuffle=True)\n    # Iteratorの定義\n    train_iter = chainer.iterators.SerialIterator(train, batchsize)\n    valid_iter = chainer.iterators.SerialIterator(valid, batchsize, repeat=False, shuffle=False)\n    train_iter.reset()\n    valid_iter.reset()\n    best_accuracy = 0\n    print(\"starting training\")\n\n    # 学習ループの実行\n    while train_iter.epoch < num_epochs:\n\n        #　------------  学習の1イテレーション  ------------\n\n      # データの取得\n        train_batch = train_iter.next()\n        x_train, t_train = chainer.dataset.concat_examples(train_batch)\n\n      # 予測値の計算\n        y_train = model(x_train)\n\n      # ロスの計算\n        loss_train = F.softmax_cross_entropy(y_train, t_train)\n\n      # 勾配の計算\n        model.cleargrads()\n        loss_train.backward()\n\n      # パラメータの更新\n        optimizer.update()\n\n      # 検証データで精度を計算\n        accuracy_train = F.accuracy(y_train, t_train)\n        accuracy_train = float(accuracy_train.data)\n\n        #  ----------------  ここまで  ----------------\n\n        if train_iter.is_new_epoch: # 新しいエポックに入った時のみ計算\n\n            valid_accuracies, valid_losses = [], []\n            while True:\n                # 検証データの取得\n                valid_batch = valid_iter.next()\n                x_valid, t_valid = chainer.dataset.concat_examples(valid_batch)\n\n             # 検証用データで順伝播の計算を実行\n                with chainer.using_config('train', False), chainer.using_config('enable_backprop', False):\n                    y_valid = model(x_valid)\n\n              # 検証データで損失関数を計算\n                loss_valid = F.softmax_cross_entropy(y_valid, t_valid)\n                valid_losses.append(float(loss_valid.data))\n\n              # 検証データで精度を計算\n                accuracy_valid = F.accuracy(y_valid, t_valid)\n                valid_accuracies.append(float(accuracy_valid.data))\n\n                if valid_iter.is_new_epoch: # 追加：1エポック計算し終わると、イテレーターをリセット\n                    valid_iter.reset()\n                    break\n\n            # Exponentialの適応\n            if (train_iter.epoch) % lr_shift_timing == 0:\n                try:\n                    optimizer.alpha = optimizer.alpha * lr_shift_rate\n                    print(optimizer.alpha)\n                except:\n                    optimizer.lr = optimizer.lr * lr_shift_rate\n                    print(optimizer.lr)\n                    \n         # 結果を表示\n            run.log('best_val_acc', np.mean(valid_accuracies))\n            print('valid_loss:{:.04f} valid_accuracy:{:.04f}'.format(np.mean(valid_losses), np.mean(valid_accuracies)))\n            print('---')\n            \n    return model\n\n# 学習を実行する関数\ndef fine_tune_model(device, comm, num_epochs, data_dir, learning_rate, optimizer_name, batchsize, lr_shift_timing, lr_shift_rate):\n    # ハイパーパラメータの観測\n    run.log('lr', np.float(learning_rate))\n    run.log('opt_name', str(optimizer_name))\n    run.log('batchsize', int(batchsize))\n\n    # インスタンス化\n    reset_seed(0)\n    model = GoogleNet()\n    if device >= 0:\n        chainer.cuda.get_device_from_id(device).use()\n        model.to_gpu(device)\n        print(\"1, passed the model to gpu\")\n    else:\n        print(\"2, didn't passe the model to gpu\")\n    # Optimizerの定義とmodelとの紐づけ\n    if 'SGD' == optimizer_name:\n        optimizer = chainer.optimizers.SGD(lr=learning_rate)\n    elif 'MomentumSGD' == optimizer_name:\n        optimizer = chainer.optimizers.MomentumSGD(lr=learning_rate)\n    else:\n        optimizer = chainer.optimizers.Adam(alpha=learning_rate)\n    optimizer = chainermn.create_multi_node_optimizer(optimizer, comm).setup(model)\n    model.base.disable_update() # 学習済みモデル部分の学習を行わない\n    # 学習の実行\n    model = train_model(model, comm, optimizer, data_dir, batchsize, num_epochs, learning_rate, lr_shift_timing, lr_shift_rate)\n    return model\n\ndef main():\n    print(\"Chainer version:\", chainer.__version__)\n\n    # get command-line arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_folder', type=str, dest='data_folder', help='data folder mounting point')\n    parser.add_argument('--num_epochs', type=int, default=20,help='number of epochs to train')\n    parser.add_argument('--output_dir', type=str, help='output directory')\n    parser.add_argument('--communicator', type=str, default='hierarchical', help='Type of communicator')\n    parser.add_argument('--learning_rate', type=float,default=0.01, help='learning rate')\n    parser.add_argument('--optimizer_name', type=str,default='SGD', help='name of optimizer')\n    parser.add_argument('--batchsize', type=int, default=64, help='batchsize')\n    parser.add_argument('--lr_shift_timing', type=int, default=6, help='Exponential shigt timing by epoch')\n    parser.add_argument('--lr_shift_rate', type=float, default=0.01, help='Exponential shigt ratio by epoch')\n    args = parser.parse_args()\n    \n    comm = chainermn.create_communicator(args.communicator)\n    device = comm.intra_rank\n    print(\"communicator is {}, device is{}\".format(comm, device))\n    \n    print(\" insidedata_folder: \" + args.data_folder)\n    data_dir = get_data(args.data_folder)\n    print(\"data directory is: \" + data_dir)\n    model = fine_tune_model(device, comm, args.num_epochs, data_dir, args.learning_rate, args.optimizer_name, args.batchsize, args.lr_shift_timing, args.lr_shift_rate)\n    os.makedirs(args.output_dir, exist_ok=True)\n    model.to_cpu()\n    chainer.serializers.save_npz(os.path.join(args.output_dir, 'model.npz'), model)\n\nif __name__ == \"__main__\":\n    main()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "モデルを定義したスクリプトを準備しておきます。  "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile chainer-doc_cat/model.py\n\nimport numpy as np\nimport chainer\nimport chainer.links as L\nimport chainer.functions as F\n\nclass GoogleNet(chainer.Chain):\n\n    def __init__(self, n_out=2):\n        super(GoogleNet, self).__init__()\n        with self.init_scope():\n            self.base = L.GoogLeNet()\n            self.fc = L.Linear(None, n_out)\n\n    def forward(self, x):\n        h = self.base(x, layers=['pool5'])\n        h = self.fc(h['pool5'])\n        return h",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Experimentの作成\nワークスペースですべての実行結果を追跡するために[Experiment](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#experiment) を作成します。  "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Experiment\n\nexperiment_name = 'chainer-dog-cat'\nexperiment = Experiment(ws, name=experiment_name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Chainer estimatorの作成\n\nAzure ML SDKの Chainer estimatorを使用すると、単一ノードと分散の両方の実行について、Chainerトレーニングジョブを簡単に送信できます。   \nChainer estimatorの詳細については、[こちら](https://docs.microsoft.com/ja-jp/python/api/azureml-train-core/azureml.train.dnn.chainer?view=azure-ml-py)を参照してください。次のコードは単一ノードのChainerジョブを定義します。\n\nまた、DataStore を学習スクリプトからアクセスさせるための詳細については[こちら](https://docs.microsoft.com/ja-jp/azure/machine-learning/service/how-to-access-data)を参照してください。\n\n先ほどの PyTouch の Estimater を呼び出すコードと、ほとんど差がない点を確認します。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.estimator import *\nfrom azureml.train.dnn import *\n\nscript_params = {\n    '--data_folder': ds.as_mount(),\n    '--num_epochs': 25,\n    '--output_dir': './outputs',\n    '--communicator': 'non_cuda_aware'\n}\n\nestimator = Chainer(source_directory=project_folder,\n                      compute_target=compute_target,\n                      script_params=script_params,\n                      entry_script='train.py',\n                      distributed_backend='mpi',     \n                      pip_packages=['cupy-cuda90', 'mpi4py', 'cython', 'chainer==5.1', 'chainermn', 'chainercv'],\n                      use_gpu=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 学習の実行\n\n下記のコードで学習を実行します。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run = experiment.submit(estimator)\nprint(run)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 学習の経過の確認\n\n学習の実行には下記の4ステップがあります。  \n\n1. 準備：Chainer Estimater で指定されたPython環境に合わせてdockerイメージが作成され、それがワークスペースのAzure Container Registryにアップロードされます。このステップはPython環境ごとに一度だけ起こります。（その後の実行のためにコンテナはキャッシュされます。）画像の作成とアップロードには約5分かかります。ジョブの準備中、ログは実行履歴にストリーミングされ、イメージ作成の進行状況を監視するために表示できます。\n\n2. スケーリング：計算をスケールアップする必要がある場合（つまり、バッチAIクラスターで現在実行可能な数より多くのノードを実行する必要がある場合）、クラスターは必要な数のノードを使用可能にするためにスケールアップを試みます。スケーリングは通常約5分かかります。\n\n3. 実行中：スクリプトフォルダ内のすべてのスクリプトがコンピューティングターゲットにアップロードされ、データストアがマウントまたはコピーされてentry_scriptが実行されます。ジョブの実行中は、stdoutと./logsフォルダが実行履歴にストリーミングされ、実行の進行状況を監視するために表示できます。\n\n4. 後処理：実行の./outputsフォルダが実行履歴にコピーされます。"
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# to get more details of your run\nprint(run.get_details())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\n\nRunDetails(run).show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run.wait_for_completion(show_output=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "手持ちのデータを用いて学習を実行することができました。  \nですが、現在はGPUマシンを1台立てて、学習を行っただけです。  \n\n続いてはGPUマシンを複数使用して、ハイパーパラメータの自動チューニングを確認します。  "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## ハイパーパラメーターのチューニング\n\n基本的に必要な設定は下記になります。  \n\n- ハイパーパラメータを探索する範囲を選択\n- ハイパーパラメータを探索する方法を選択（ランダムスイープ、グリッドサーチ、ベイズ最適化）\n- 最大化or最小化したいメトリックの指定\n\n詳細の設定方法については[こちらの公式ドキュメント](https://docs.microsoft.com/ja-jp/azure/machine-learning/service/how-to-tune-hyperparameters)を確認してください。  \n\n\n### 学習率の調整\n\nトレーニングスクリプト（`train.py`）は数エポックごとに学習率を減少させるために学習率スケジュールを使うので、最初の学習率とmomentumのパラメータを調整しましょう。  \n今回はランダムスイープを使用します。  \n（この節でてくる語句については[公式ドキュメント](https://docs.microsoft.com/ja-jp/azure/machine-learning/service/how-to-access-data)を確認してください。）\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.hyperdrive import *\n\nparam_sampling = RandomParameterSampling( {\n    'learning_rate':uniform(0.005, 0.01),\n    'optimizer_name': choice('SGD', 'MomentumSGD', 'Adam'),\n    'batchsize': choice(16, 32, 64),\n    }\n)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 早期終了(Early Stopping)の適応\n\n次に、パフォーマンスの低い実行を早期終了するための設定を行います。  \n\nここでは `BanditPolicy`を使います。これは主要なすべての実行を終了します。  \n設定はシンプルに3つの要素を指定します。\n\n- slack_factor：早期終了を行う閾値（指定したメトリックの最高のものとの誤差）\n- evaluation_interval：早期終了を確認する周期（エポック数）\n- delay_evaluation：早期終了を確認を始めるタイミング（0エポック目から何エポック遅らせるか）\n\nここでは、ポリシーをエポックごとに適用します（エポックごとに `best_val_acc`メトリックをレポートし、` evaluation_interval = 1`になるため）  \n最初の `10`エポック（` delay_evaluation = 10`）の後まで最初のポリシー評価を遅らせることに注意してください。\nBanditPolicyおよびその他のポリシーの詳細については、[ここ](https://docs.microsoft.com/azure/machine-learning/service/how-to-tune-hyperparameters#specify-an-early-termination-policy)を参照してください。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "early_termination_policy = BanditPolicy(slack_factor=0.15, evaluation_interval=1, delay_evaluation=10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 設定の定義\n\n設定には`HyperDriveRunConfig()`を使用します。  \n詳細は[こちらの公式ドキュメント](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriverunconfig?view=azure-ml-py)を確認してください。  \n\n`primary_metric_name='best_val_acc'`で検証データへの精度を評価指標として使用する事を決定し、`primary_metric_goal=PrimaryMetricGoal.MAXIMIZE`でその値が最大となる事を目指すと設定しています。  \n`MINIMIZE`と指定することによって、最小にする事を目指すことも可能です。  \n\nまた`max_total_runs=4`で学習実行数を指定、`max_concurrent_runs=8`で同時に学習を行う数を指定しています。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "hyperdrive_run_config = HyperDriveRunConfig(estimator=estimator,\n                                            hyperparameter_sampling=param_sampling, \n                                            policy=early_termination_policy,\n                                            primary_metric_name='best_val_acc',\n                                            primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n                                            max_total_runs=8,\n                                            max_concurrent_runs=4)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "これで設定が完了したので、早速学習を行ってみましょう。\n\n学習時の環境にもよりますが、並列度 4で、**10分程度**かかります。\n\n初期設定の通り、並列度 4、最大8、AmlCompute 4の場合、20-25分程度かかります。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# HyperDrive runの開始\nhyperdrive_run = experiment.submit(hyperdrive_run_config)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\n\nRunDetails(hyperdrive_run).show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "hyperdrive_run.wait_for_completion(show_output=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 最適なモデルの取得\n\n\n\nすべての実行が完了すると、モデルを最も正確に生成した実行を見つけることができます。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "best_run = hyperdrive_run.get_best_run_by_primary_metric()\nbest_run_metrics = best_run.get_metribcs()\nprint(best_run)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "最後に、最も優れた実行結果からモデルをワークスペースに登録します。   \n`model_path`パラメータはあなたの` output`ディレクトリのモデルファイルへのリモートVM上の相対パスを取ります。次のセクションでは、この登録モデルをWebサービスとしてデプロイします。"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 最適なモデルのデプロイ"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model = best_run.register_model(model_name = 'Chainer-dogcat', model_path = 'outputs/model.pt')\nprint(model.name, model.id, model.version, sep = '\\t')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 以降、開発中!!!\n\nここ以降まだ確認できていません。"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### スコアリングスクリプトの作成\n\n- `init（）`：この関数では、通常モデルを `global`オブジェクトにロードします。この関数はDockerコンテナが起動されたときに一度だけ実行されます。\n- `run（input_data）`：この関数では、新たな入力データ対して学習済みモデルを使用して推論を実行します。通常は入力と出力は通常シリアライゼーションとデシリアライゼーションのフォーマットとしてJSONを使用しますが、他のフォーマットも使用することが可能です。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile chainer-doc_cat/score.py\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms\nimport json\n\nfrom azureml.core.model import Model\n\n\ndef init():\n    global model\n    model_path = Model.get_model_path('pytorch-dogcat')\n    model = torch.load(model_path, map_location=lambda storage, loc: storage)\n    model.eval()\n\n\ndef run(input_data):\n    input_data = torch.tensor(json.loads(input_data)['data'])\n\n    # get prediction\n    with torch.no_grad():\n        output = model(input_data)\n        classes = ['cat', 'dog']\n        softmax = nn.Softmax(dim=1)\n        pred_probs = softmax(output).numpy()[0]\n        index = torch.argmax(output, 1)\n\n    result = {\"label\": classes[index], \"probability\": str(pred_probs[index])}\n    return result",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 環境ファイルを作成する\n\nスコアリングスクリプトのすべてのパッケージ依存関係を指定する環境ファイル（ `myenv.yml`）を作成する必要があります。このファイルは、Azure MLによってこれらのすべての依存関係がDockerイメージにインストールされるようにするために使用されます。この場合、 `azureml-core`、` torch`、そして `torchvision`が必要になります。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.conda_dependencies import CondaDependencies \n\nmyenv = CondaDependencies.create(pip_packages=['azureml-defaults', 'torch', 'torchvision'])\n\nwith open(\"myenv.yml\",\"w\") as f:\n    f.write(myenv.serialize_to_string())\n    \nprint(myenv.serialize_to_string())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Dockerイメージの設定\n\nACIコンテナーを構築するために使用するDockerイメージを構成します。  \n詳細については[こちらの公式ドキュメント](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.image.containerimage?view=azure-ml-py)を確認してください。  "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.image import ContainerImage\n\nimage_config = ContainerImage.image_configuration(execution_script='pytorch_score.py', \n                                                  runtime='python', \n                                                  conda_file='myenv.yml',\n                                                  description='Image with hymenoptera model')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### ACIコンテナの設定\n\nデプロイのための準備がほぼ整いました。   \nACIコンテナに必要なCPUの数とギガバイトのRAMを指定するためのデプロイメント構成ファイルを作成します。  \nそれは作成したモデルに依存しますが、一般的なモデルではデフォルトの `1`コアと` 1`ギガバイトのRAMで十分なケースが多いです。  "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.webservice import AciWebservice\n\naciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n                                               memory_gb=1, \n                                               tags={'data': 'dog_cat',  'method':'transfer learning', 'framework':'chainer'},\n                                               description='Classify dogs/cats using transfer learning with Chainer')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.webservice import AciWebservice\n\naciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n                                               memory_gb=1, \n                                               tags={'data': 'dogcat',  'method':'transfer learning', 'framework':'pytorch'},\n                                               description='Classify ants/bees using transfer learning with PyTorch')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Container Instances にデプロイする\n\n最後に、登録したモデルからWebサービスをデプロイしましょう。  \n前の手順で作成したACI設定ファイルとイメージ設定ファイルを使用してWebサービスをデプロイします。  \n\nリストの中の `model`オブジェクトを` models`パラメータに渡します。  \n複数の登録済みモデルをデプロイする場合は、このリストに他のモデルを追加してください。　　"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%time\nfrom azureml.core.webservice import Webservice\n\nservice_name = 'classify-dog-cat'\nservice = Webservice.deploy_from_model(workspace=ws,\n                                       name=service_name,\n                                       models=[model],\n                                       image_config=image_config,\n                                       deployment_config=aciconfig,)\n\nservice.wait_for_deployment(show_output=True)\nprint(service.state)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "通常デプロイには7~8分かかります。  \n下記のように表示されればデプロイが成功しています。  \n\n```\nSucceededACI service creation operation finished, operation \"Succeeded\"\n\n```\n\n#### デプロイがうまくいかない場合\n\nもし、何らかの理由でデプロイが失敗して再デプロイする必要がある場合は、必ずサービスを`service.delete（）`で削除してください。  \n\n**また、デプロイに問題が発生した場合、まず下記のコマンドを実行して、サービスからログを取得しましょう。**  \n\n`service.get_logs()`"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## デプロイされたサービスをテストする\n\n最後に、デプロイしたWebサービスをテストしましょう。  \nデータをJSON文字列としてACIでホストされているWebサービスに送信し、SDKの `run` APIを使用してサービスを呼び出します。  \nここで、検証データからイメージを取得して推論を実行します。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os, json\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nplt.imshow(Image.open('test_img2.jpeg'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "画像データに対して学習時と同じ前処理を適応し、推論が実行できる状態に変更します。  "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import torch\nfrom torchvision import transforms\n    \ndef preprocess(image_file):\n    \"\"\"Preprocess the input image.\"\"\"\n    data_transforms = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n\n    image = Image.open(image_file)\n    image = data_transforms(image).float()\n    image = torch.tensor(image)\n    image = image.unsqueeze(0)\n    return image.numpy()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "input_data = preprocess('test_img2.jpeg')\nresult = service.run(input_data=json.dumps({'data': input_data.tolist()}))\nprint(result)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "うまく推論できていることが確認できました。  \nこれで自身のデータに対してもAzure MLを用いて、GPUクラスタを用いてのハイパーパラメータの調整、そしてデプロイまでを行うことのできる力がつきました。\n\n## 後片付け\n\nWebサービスが不要になったら、API呼び出しで簡単に削除しておきましょう。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "service.delete()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "これでハンズオンは終了です。  \n皆さんお疲れ様でした。"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}